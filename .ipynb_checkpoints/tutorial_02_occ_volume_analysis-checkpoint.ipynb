{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutoral 2 - OCC Volume Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This purpose of this analysis is to produce a volume-by-underlying report from a data file sourced from the OCC.  The data in the OCC file is for the month of August 2018.  However, it is far more granular than we need, so we will need to group and summarize (a very common task in data analysis).  \n",
    "\n",
    "Along the way we will discuss basic features of the `pandas` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` package contains much of the data wrangling functionality that we will need.  For those of you who are familiar with R, you can thing of `pandas` as the Python equivalent of R's `tidyverse`, however `pandas` has a larger scope than the core tidyverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In theory you can give *pandas* any alias that you want, but it would be *highly* non-pythonic to call it anything other than **pd**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to make a couple of changes to the way that the notebook behaves (both of these are largely a matter of preference). This first bit of code changes the maximimun number of rows that will be displayed when we print a `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> pd.options.display.max_rows = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make it so that the output of every line of code in a cell is printed.  The default behavior is that only the last line of code is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> from IPython.core.interactiveshell import InteractiveShell\n",
    "##> InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading In Data from a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` library has a `.read_csv()` method that will read in a table of data from a CSV and put it in a `DataFrame`, which is the main data object in `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ = pd.read_csv('data/occ_option_volume_201808.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is a monthly option volume report from the OCC.  The data is broken down by trade-date, underlying, account type, puts/calls, and exchange. We can print a the contents of the `DataFrame` by simply typing its name and then running that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice at the very bottom that the total number of rows and columns has been printed.  We can also get this information from the `.shape` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `type()` function is also useful for exploring objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create a list object to test on.\n",
    "##> my_list = [1, \"twelve\", False]\n",
    "\n",
    "\n",
    "# then let's check the type of my_list as well as df_occ\n",
    "##> type(my_list)\n",
    "##> type(df_occ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the data type of each of the columns with the `.dtypes` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the string colums are given a data type of `object`.  Also notice that the `actdate` column was read in as a string, rather than a date, which we will fix later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the columns of a dataframe by use of th `[` notation as well as the `.` notation.  Let's use both of these approaches to isolate the `underlying` column of `df_occ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both of these are equivalent\n",
    "##> df_occ['underlying']\n",
    "##> df_occ.underlying\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the the `type()` function to see that a column of a `DataFrame` is a `Series`, which is a different kind of `pandas` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> type(df_occ['underlying'])\n",
    "##> type(df_occ.underlying)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't get into the weeds about this point too much, but it is worth noting that a `DataFrame` is a bunch of `Series` glued together.  For those of you familiar with R, this is similar the fact that a `data.frame` is a `list` of atomic vectors, all of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring the Date Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we saw that the `df_occ.actdate` is infact an `object` data type, rather than a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare the dtype of the df_occ.quantity and df_occ.actdate\n",
    "##> df_occ.quantity.dtype\n",
    "##> df_occ.actdate.dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `pandas.to_datetime()` function for the purposes of this refactoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> pd.to_datetime(df_occ.actdate, format='%m/%d/%Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this code above doesn't actually change `df_occ.actdate` but rather creates a new `Series` and prints it to the output.  We can test this by checking the `dtype` property of the column again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.actdate.dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to actually affect the change we are looking for, we need to reassign to `df_occ.actdate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.actdate = pd.to_datetime(df_occ.actdate, format = '%m/%d/%Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the refactoring from `object` to `date` has actually occured.  Let's check the `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration - Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you first encounter a data set, it is useful to explore the unique values in some of the columns to try to get a feel for what is in the data set.  Let's look at the `underlying` column and the `actdate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.actdate.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our dates are all in August of 2018, but it's hard to be sure from visual inspection because the priting is messy.  Let's use the `min` and `max` methods just to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.actdate.unique().min()\n",
    "##> df_occ.actdate.unique().max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next look at the unique values of `underlying` - there are a lot of them so they are not all printed.  Notice that the `.unique()` method returns an `array` object.  In fact this is a `ndarray` which is the foundation data structure of `numpy`.  The `pandas` package is built on top of `numpy`.\n",
    "\n",
    "\n",
    "Let's check the size of the array, with the `.size` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.underlying.unique()\n",
    "##> df_occ.underlying.unique().size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** How many individual dates are represented in the `df_occ`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping And Summarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ultimate objective is to know the total option volume for each underlying in the month of August 2018.  The data in it's current form is far more granular than that.  We will do some aggregation in order to get the data in the form that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do is sum up the total quantity for each underlying.  We can do this with the `groupby()` method couple with the `sum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_occ.groupby('underlying')['quantity'].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is actually a `Series` object, not a `DataFrame`. (Which is different than how `summarize()` works in `dplyr`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> type(df_occ.groupby('underlying')['quantity'].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this to a `DataFrame` using the `Series.to_frame()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_report = df_occ.groupby('underlying')['quantity'].sum().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 100 ETFs (non-volatility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we used the `.groupby()` function to get the aggregated data we wanted.  Since the backtesting we will eventually do will focus on liquid, non-volatility ETFs, let's see if we can restrict ourselves to those underlyings.  In particular, the goal of this section is to find the 100 highest volume, non-volatility, ETFs.\n",
    "\n",
    "We will start by reading in a master list of ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf = pd.read_csv('data/etf_list.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the non-volatility ETFs we will perform a filtering operation using the segment column.  But first we will need to lowercase all the letters in that column.  We use the `Series.str.lower()` method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf['segment'] = df_etf['segment'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the `Series.str.contains()` method to isolate all the non-volatility ETFs.  Notice that this utilizes boolean indexing of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_non_vol_etf = df_etf[~df_etf['segment'].str.contains('volatility')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pandas`, both `Series` and `DataFrame` objects have an index. Among other things, an `index` both names and orders the entries of at `Series` or `DataFrame`.  Indices are also used when joining together two dataframes.\n",
    "\n",
    "We can view the index by using the `DataFrame.index` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_non_vol_etf.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the index of `df_non_vol_etf` is an index of integers.\n",
    "\n",
    "In order to affect the join that we want, we will reindex `df_non_vol_etf` with the `DataFrame.set_index()` method.  The following code drops the integer index and turns the `symbol` column into an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_non_vol_etf.set_index('symbol', inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataframe to make sure the change took place as we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_non_vol_etf.index\n",
    "##> df_non_vol_etf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to perform an inner-join on `df_non_vol_etf` and `df_report`.  The result of this join is that only underlyings that exist in both dataframes remain.  By default the `.join()` uses the indexes of the two dataframes. The resulting joined datframes has the columns from both the original dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_non_vol_etf.join(df_report, how = 'inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that what we want is the top 100 most liquid ETFs.  In order to do this, we first sort with `DataFrame.sort_values()` and then use `.head()` to grab the top 100 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_non_vol_etf.join(df_report, how = 'inner').sort_values('quantity', ascending = False).head(100)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
