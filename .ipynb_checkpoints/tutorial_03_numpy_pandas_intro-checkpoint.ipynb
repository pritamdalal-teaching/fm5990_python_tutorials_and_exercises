{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03 - Introduction to `numpy` and `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python is becoming a de facto standard programming language in science and data analysis.\n",
    "\n",
    "- However, this was not always the case.  Python was not initially designed for data analysis (unlike R and Matlab, which both were).\n",
    "\n",
    "- Python became a scientific computing workhorse through the development of two packages:\n",
    "    - `numpy`\n",
    "    - `pandas`\n",
    "\n",
    "- The purpose of this tutorial is to introduce these two packages, and along the way to take a first look at some financial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's start by importing the packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import numpy as np\n",
    "##> import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these packages have a lot of functionality, but here is what they do in brief:\n",
    "- `numpy`: vector and matrix computation (similar to what is native in R and Matlab)\n",
    "- `pandas`: dataframe data structure that allows for analysis of data (like R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `numpy.array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have already discussed the `list` structure, which is Python's simplest and most flexible way of storing multiple values in a single variable.\n",
    "\n",
    "- However, this flexibility comes at a cost of performance: large lists are very slow.\n",
    "\n",
    "- The `array` structure in the numpy package can be thought of as a vector or a matrix, and allows for efficient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to create an array is by starting with a list and then use the `np.array()` method.  \n",
    "\n",
    "Let's try the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> l = [1, 2, 3]\n",
    "##> arr1 = np.array(l)\n",
    "##> arr2 = np.array([1, 2, 3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the types of the variables we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(type(l))\n",
    "##> print(type(arr1))\n",
    "##> print(type(arr2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the content of what's inside our two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(arr1)\n",
    "##> print(arr2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you print an array to the console, this is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> arr1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We typically won't work with arrays directly, or have to build them from scratch.\n",
    "- Usually, we will be working with them indirectly, since `pandas` dataframe are built on top of them.\n",
    "- It's good to know `arrays` exist, and to realize that `numpy` is what makes scientific computing possible in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas.DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `DataFrame` structure from the `pandas` package is going to be our primary workhorse.\n",
    "\n",
    "- A `DataFrame` is a convenient way to store rectangular data that consists of rows and columns.\n",
    "\n",
    "- Usually, the data that goes in a `DataFrame` come from an external source.  \n",
    "\n",
    "- In this class, our data will usually come from special text files, called CSV files, and will be read into a `DataFrame` via the `pandas.read_csv()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in our first data set into a `DataFrame` by typing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy = pd.read_csv(\"data/spy_dec_2018.csv\")\n",
    "##> df_spy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This dataframe consists of all the December end-of-day prices for SPY (which is an ETF that tracks the S&P500 Index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the `df_spy` dataframe that we have just created.  \n",
    "\n",
    "This is a very typical thing to do once you've loaded a new dataset.\n",
    "\n",
    "First, we can first use the `type()` method to make sure what we have created is in fact a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> type(df_spy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the `.dtypes` attribute of the `DataFrame` to see the data types of each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of rows and columns by using the `.shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, our dataframe `df_spy` consists of 18 row and 7 columns.\n",
    "\n",
    "- A `DataFrame` can be thought of as a collection of rows and a collection of columns.  Both view points are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to isolate a particular column, say the `date` column, we can use square brackets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['close']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the following code, each column of a `DataFrame` is actually a different kind of `pandas` structure called a `Series`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> type(df_spy['close'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a bit of `pandas` nerdery (don't get too bogged down with these details):\n",
    "- A `DataFrame` is really a collection of columns glued together, and each column is a `Series`.\n",
    "- A pandas `Series` has two major components: 1) `.values`; 2) `.index`.\n",
    "- The `.values` of a `Series` is a `numpy.array`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `.values` attribute of the `close` column of `df_spy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['close'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component-wise Column Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we saw in the previous section, a `DataFrame` column is essentially a fancy `numpy.array`.\n",
    "\n",
    "- We can think of a `numpy.array` as a vector in Python.\n",
    "\n",
    "- As such, we can perform vector-like calculations with `DataFrame` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform scalar arithmetic on the entire column, and the calculation *broadcasts* as we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['close'] / 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['close'] + 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform component-wise calculations between two colums.\n",
    "\n",
    "Let's say we want to calculate the intraday range of SPY for each of the trade-dates in `df_spy`.  This is the difference between the `high` and the `low` of each day.  We can do this easily from the columns of our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['high'] - df_spy['low']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Columns to a `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data analysis often involves starting with a `DataFrame` and then adding new columns to it, which are functions of the existing columns.\n",
    "\n",
    "- Continuing our previouse example, let's say we want to save our intraday ranges back into `df_spy` for further analysis later.\n",
    "\n",
    "We can do this easily with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['intraday_range'] = df_spy['high'] - df_spy['low']\n",
    "##> df_spy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that we sort of assumed that `intraday_range` exists and assign a value to it (which is a function of two of our existing variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Aggregating Calulations on `DataFrames`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have already seen how to do *component-wise* calculations on the column of a `DataFrame`.\n",
    "\n",
    "- *Component-wise* calculations can be thought of as a function that takes in vectors and scalars, and returns a scalar.\n",
    "\n",
    "- It is often useful to perform *aggregation* calculations on `DataFrame`.\n",
    "\n",
    "- An *aggregation* can be thought of a function that takes in a vector, and returns a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can use the following code to calculate the total SPY volume during December:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_spy['volume'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we did here:\n",
    "- First, we isolated the `volume` column of the DataFrame, which is a `Series` (a souped up `array`).\n",
    "- A `numpy.array` has method called `.sum()` which adds up all the components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate some summary statistics on the `intraday_range` column that we added to our dataframe earlier.  Notice that each of these summary statistics is just an aggregating calcuation on the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> print(\"Mean: \", df_spy['intraday_range'].mean()) # average\n",
    "##> print(\"St Dev:\", df_spy['intraday_range'].std()) # standard deviation\n",
    "##> print(\"Min:\" , df_spy['intraday_range'].min()) # minimum\n",
    "##> print(\"Max:\" , df_spy['intraday_range'].max()) # maximum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PDSH* - Section 3.1 - Introducing Pandas Objects\n",
    "\n",
    "*PDSH* - Section 2.1 - Understanding Data Types in Python\n",
    "\n",
    "*PDSH* - Section 2.2 - The Basics of NumPy Arrays\n",
    "\n",
    "*PDSH* - Section 2.3 - Computation on NumPy Arrays: Universal Functions\n",
    "\n",
    "*PDSH* - Section 2.4 - Aggregations: Min, Max, and Everything In Between"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
