{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 09 - Grouping and Aggregating - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real power of data analysis with `DataFrames` comes into focus when we start utilizing the `.groupby()` and `.agg()` methods.  This is known as grouping and aggegregating.\n",
    "\n",
    "Talking about grouping in the abstract can be confusing; I think it's best to see grouping in action by doing meaningful calculations.\n",
    "\n",
    "The purpose of this tutorial is to introduce grouping and aggregation by way of completing a finance related task: calculating monthly returns and volatilies for several assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the packages that we will need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> import numpy as np\n",
    "##> import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis we will be performing will be on the set of of December 2018 prices for `SPY`, `IWM`, `QQQ`, `DIA`.\n",
    "\n",
    "Let's read that data in from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf = pd.read_csv(\"../data/index_etf_dec_2018.csv\")\n",
    "##> df_etf.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Challenge:** Use dataframe masking to isolate all the `QQQ` prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Daily Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bit of code calculates the daily return for each day and each symbol.  It's similar to the code we used in a previous tutorial except there is some logic to deal with the fact that there are different symbols in the same data set.  For the purposes of this tutorial you can simply run the cod.  If you are curious about it, I encourage you to analyze it on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjusted</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>280.279999</td>\n",
       "      <td>280.399994</td>\n",
       "      <td>277.510010</td>\n",
       "      <td>279.299988</td>\n",
       "      <td>103176300</td>\n",
       "      <td>277.678436</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>278.369995</td>\n",
       "      <td>278.850006</td>\n",
       "      <td>269.899994</td>\n",
       "      <td>270.250000</td>\n",
       "      <td>177986000</td>\n",
       "      <td>268.681000</td>\n",
       "      <td>-0.032402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>265.920013</td>\n",
       "      <td>269.970001</td>\n",
       "      <td>262.440002</td>\n",
       "      <td>269.839996</td>\n",
       "      <td>204185400</td>\n",
       "      <td>268.273376</td>\n",
       "      <td>-0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>269.459991</td>\n",
       "      <td>271.220001</td>\n",
       "      <td>262.630005</td>\n",
       "      <td>263.570007</td>\n",
       "      <td>161018900</td>\n",
       "      <td>262.039795</td>\n",
       "      <td>-0.023236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPY</td>\n",
       "      <td>2018-12-10</td>\n",
       "      <td>263.369995</td>\n",
       "      <td>265.160004</td>\n",
       "      <td>258.619995</td>\n",
       "      <td>264.070007</td>\n",
       "      <td>151445900</td>\n",
       "      <td>262.536896</td>\n",
       "      <td>0.001897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol        date        open        high         low       close  \\\n",
       "0    SPY  2018-12-03  280.279999  280.399994  277.510010  279.299988   \n",
       "1    SPY  2018-12-04  278.369995  278.850006  269.899994  270.250000   \n",
       "2    SPY  2018-12-06  265.920013  269.970001  262.440002  269.839996   \n",
       "3    SPY  2018-12-07  269.459991  271.220001  262.630005  263.570007   \n",
       "4    SPY  2018-12-10  263.369995  265.160004  258.619995  264.070007   \n",
       "\n",
       "      volume    adjusted    return  \n",
       "0  103176300  277.678436       NaN  \n",
       "1  177986000  268.681000 -0.032402  \n",
       "2  204185400  268.273376 -0.001517  \n",
       "3  161018900  262.039795 -0.023236  \n",
       "4  151445900  262.536896  0.001897  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_etf['return'] = np.nan\n",
    "for ix in range(1, df_etf.shape[0]):\n",
    "    \n",
    "    # grabbing symbols from df_etf\n",
    "    curr_sym = df_etf.at[ix, 'symbol']\n",
    "    prev_sym = df_etf.at[ix-1, 'symbol']\n",
    "    \n",
    "    # grabbling prices from df_etf\n",
    "    curr_adj = df_etf.at[ix, 'adjusted']\n",
    "    prev_adj = df_etf.at[ix-1, 'adjusted']\n",
    "    \n",
    "    # calculating return\n",
    "    if curr_sym == prev_sym:\n",
    "        df_etf.at[ix, 'return'] = (curr_adj / prev_adj) - 1\n",
    "\n",
    "df_etf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first date in our data set is December 3, 2018.  As a simple sanity check, let's utilize dataframe masking to make sure that all those returns are set as `NaN`, since the return is not defined on the first day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf[df_etf.date==\"2018-12-03\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sanity check looks good.\n",
    "\n",
    "We'll now proceed to calculating monthly returns and volatility for each of the ETFs in our data set.  This amounts to first grouping on `symbol`, and then aggregating the `returns`.  \n",
    "\n",
    "Let's start with monthly return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Return for Each `symbol`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at our data set, we now have daily returns for each day, for each of the four ETFs.  Given a bit of time, you could probably come up with a `for` loop to iterate through the `DataFrame` and produce the monthly returns for each ETF (in fact, you could modify the returns `for` loop from the previous section to do just that).  As a one-off solution, that would be fine.  But grouping and aggregating are so ubiquitous that it would be inconvenient to have write a `for`-loop every time.\n",
    "\n",
    "\n",
    "Let's begin by calculating the daily growth factor in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_etf['growth_factor'] = 1 + df_etf['return']\n",
    "##> df_etf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the monthly growth factor is the product of the daily growth factor - that's exactly what you would code up in a `for` loop.  Here is a way to write all that logic in a single line using `.groupby()` and `.agg()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_factor = \\\n",
    "##>     df_etf.groupby(['symbol'])['growth_factor'].agg([np.prod]).reset_index()\n",
    "##> df_grouped_factor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `pandas` isn't very sophisticated about the name that it gives to the column that stores the aggregation calculation column.  It just gave it the name `prod`, which the name of the function that was used in the aggregation calculation.  Let's make `df_grouped_factor` a bit more readable by renaming that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_factor.rename(columns={'prod': 'monthly_factor'}, inplace=True)\n",
    "##> df_grouped_factor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, recall that the monthly return is gotten by subracting one from the monthly growth factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_factor['monthly_return'] = df_grouped_factor['monthly_factor'] - 1\n",
    "##> df_grouped_factor[['symbol', 'monthly_return']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Volatility for Each `symbol`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the (realized/historical) volatility for each of the ETFs.  Recall that volatility is the standard deviation of the daily returns.  If we were to do this in a brute force manner, we could use dataframe masking to separate out the returns for each of the four ETFs, and then calculate four separate standarde deviations.\n",
    "\n",
    "However, once again with the power of `.groupby()` and `.agg()` we can do all of this with a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_vol = \\\n",
    "##>     df_etf.groupby(['symbol'])['return'].agg([np.std]).reset_index()\n",
    "##> \n",
    "##> df_grouped_vol\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's rename our aggregation column to something more descriptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_vol.rename(columns={'std':'daily_vol'}, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have calculated is a daily volatility, but when practitioners talk about volatility, the typically annualize it.  A daily volatility is annualized by multiplying by $\\sqrt{252}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##> df_grouped_vol['ann_vol'] = df_grouped_vol['daily_vol'] * np.sqrt(252)\n",
    "##> df_grouped_vol\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Use `groupby()` and `.agg()` to calculated the average daily return for each of the ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Reading\n",
    "\n",
    "*PDSH* - 3.8 - Aggregation and Grouping\n",
    "\n",
    "*Python for Data Analysis (McKinney)* - Chapter 9 (pp 251-274) Data Aggregation and Grouping Operations\n",
    "\n",
    "*Options, Futures, and Other Derivatives (Hull)* - Chapter 15 (pp 325-329) The Black-Scholes-Merton Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
